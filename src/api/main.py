"""
API FastAPI pour la Pr√©diction de Churn Bancaire
=================================================

Endpoints:
- POST /predict : Pr√©diction individuelle
- POST /predict/batch : Pr√©dictions multiples
- GET /health : Health check
- GET /metrics : M√©triques du mod√®le

"""

from fastapi import FastAPI, HTTPException, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import pandas as pd
import numpy as np
import joblib
import json
import time
from datetime import datetime
from pathlib import Path
import logging
import sys

# Import des sch√©mas
from .schemas import (
    CustomerFeatures, PredictionResponse, BatchPredictionRequest,
    BatchPredictionResponse, HealthResponse, ModelMetrics, ErrorResponse
)

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Initialisation de l'application
app = FastAPI(
    title="Bank Churn Prediction API",
    description="API de pr√©diction du churn bancaire avec MLOps",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# Configuration CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Variables globales
model = None
preprocessor = None
model_metadata = None
start_time = time.time()


def load_model_and_preprocessor():
    global model, preprocessor, model_metadata

    # Liste des chemins √† tester (ordonn√©s par priorit√©)
    model_paths = [
        '/Users/denismutombotshituka/bank-churn-mlops/models/trained/model_latest.pkl',
        '/app/models/trained/model_latest.pkl'
    ]
    preprocessor_paths = [
        '/Users/denismutombotshituka/bank-churn-mlops/src/models/preprocessor.pkl',
        '/app/src/models/preprocessor.pkl'
    ]
    metadata_paths = [
        '/Users/denismutombotshituka/bank-churn-mlops/models/model_metadata.json',
        '/app/models/model_metadata.json'
    ]

    # Trouver le premier chemin existant pour chacun
    model_path = next((p for p in model_paths if Path(p).exists()), None)
    preprocessor_path = next((p for p in preprocessor_paths if Path(p).exists()), None)
    metadata_path = next((p for p in metadata_paths if Path(p).exists()), None)

    try:
        if model_path is None:
            raise FileNotFoundError(f"Aucun mod√®le trouv√© dans {model_paths}")
        if preprocessor_path is None:
            raise FileNotFoundError(f"Aucun preprocessor trouv√© dans {preprocessor_paths}")
        
        logger.info(f"Chargement du mod√®le depuis {model_path}")
        model = joblib.load(model_path)
        logger.info("‚úÖ Mod√®le charg√© avec succ√®s")

        logger.info(f"Chargement du preprocessor depuis {preprocessor_path}")
        preprocessor = joblib.load(preprocessor_path)
        logger.info("‚úÖ Preprocessor charg√© avec succ√®s")

        if metadata_path and Path(metadata_path).exists():
            with open(metadata_path, 'r') as f:
                model_metadata = json.load(f)
            logger.info("‚úÖ M√©tadonn√©es charg√©es")
        else:
            logger.warning("‚ö†Ô∏è Fichier de m√©tadonn√©es introuvable")
            model_metadata = {
                "model_name": "Unknown",
                "timestamp": datetime.now().isoformat(),
                "metrics": {}
            }
        return True

    except Exception as e:
        logger.error(f"‚ùå Erreur lors du chargement : {str(e)}")
        return False



def calculate_risk_level(probability: float) -> str:
    """
    Calculer le niveau de risque bas√© sur la probabilit√©
    
    Parameters:
    -----------
    probability : float
        Probabilit√© de churn (0-1)
        
    Returns:
    --------
    str : 'Low', 'Medium', ou 'High'
    """
    if probability < 0.3:
        return "Low"
    elif probability < 0.6:
        return "Medium"
    else:
        return "High"


def prepare_input_data(customer: CustomerFeatures) -> pd.DataFrame:
    """
    Pr√©parer les donn√©es d'entr√©e pour la pr√©diction
    
    Parameters:
    -----------
    customer : CustomerFeatures
        Donn√©es du client
        
    Returns:
    --------
    pd.DataFrame
    """
    # Convertir en DataFrame
    data = {
        'credit_score': customer.credit_score,
        'country': customer.country,
        'gender': customer.gender,
        'age': customer.age,
        'tenure': customer.tenure,
        'balance': customer.balance,
        'products_number': customer.products_number,
        'credit_card': customer.credit_card,
        'active_member': customer.active_member,
        'estimated_salary': customer.estimated_salary,
        'customer_id': customer.customer_id
    }
    
    # Ajouter colonnes optionnelles
    if customer.customer_id is not None:
        data['customer_id'] = customer.customer_id
    
    # Ajouter colonnes n√©cessaires pour le preprocessing
    data['RowNumber'] = 0
    
    return pd.DataFrame([data])


@app.on_event("startup")
async def startup_event():
    """
    √âv√©nement ex√©cut√© au d√©marrage de l'API
    """
    logger.info("=" * 60)
    logger.info("üöÄ D√âMARRAGE DE L'API CHURN PREDICTION")
    logger.info("=" * 60)
    
    success = load_model_and_preprocessor()
    
    if not success:
        logger.error("‚ùå √âchec du chargement du mod√®le")
        logger.error("L'API d√©marrera mais les pr√©dictions √©choueront")
    else:
        logger.info("‚úÖ API pr√™te √† recevoir des requ√™tes")


@app.get("/", tags=["Root"])
async def root():
    """
    Endpoint racine
    """
    return {
        "message": "Bank Churn Prediction API",
        "version": "1.0.0",
        "status": "running",
        "docs": "/docs",
        "health": "/health"
    }


@app.get("/health", response_model=HealthResponse, tags=["Health"])
async def health_check():
    """
    Health check - V√©rifier l'√©tat du service
    """
    uptime = time.time() - start_time
    
    return HealthResponse(
        status="healthy" if model is not None else "unhealthy",
        model_loaded=model is not None,
        model_version=model_metadata.get('timestamp', 'unknown') if model_metadata else 'unknown',
        uptime_seconds=round(uptime, 2),
        timestamp=datetime.now().isoformat()
    )


@app.get("/metrics", response_model=ModelMetrics, tags=["Model"])
async def get_model_metrics():
    """
    Obtenir les m√©triques du mod√®le en production
    """
    if model_metadata is None:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="M√©tadonn√©es du mod√®le non disponibles"
        )
    
    metrics = model_metadata.get('metrics', {})
    
    return ModelMetrics(
        model_name=model_metadata.get('model_name', 'Unknown'),
        accuracy=metrics.get('accuracy', 0.0),
        precision=metrics.get('precision', 0.0),
        recall=metrics.get('recall', 0.0),
        f1_score=metrics.get('f1_score', 0.0),
        roc_auc=metrics.get('roc_auc', 0.0),
        training_date=model_metadata.get('timestamp', 'unknown')
    )


@app.post("/predict", response_model=PredictionResponse, tags=["Prediction"])
async def predict_churn(customer: CustomerFeatures):
    """
    Pr√©dire le churn pour un client individuel
    
    Parameters:
    -----------
    customer : CustomerFeatures
        Donn√©es du client
        
    Returns:
    --------
    PredictionResponse
        Pr√©diction et probabilit√© de churn
    """
    start_pred = time.time()
    
    try:
        # V√©rifier que le mod√®le est charg√©
        if model is None or preprocessor is None:
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="Mod√®le non charg√©"
            )
        
        # Pr√©parer les donn√©es
        input_df = prepare_input_data(customer)
        
        # Preprocessing
        input_processed = preprocessor.transform(input_df)
        
        # Pr√©diction
        prediction = int(model.predict(input_processed)[0])
        probability = float(model.predict_proba(input_processed)[0, 1])
        
        # Calculer le niveau de risque
        risk_level = calculate_risk_level(probability)
        
        # Temps de traitement
        processing_time = (time.time() - start_pred) * 1000  # en ms
        
        logger.info(f"Pr√©diction effectu√©e en {processing_time:.2f}ms - Customer: {customer.customer_id} - Churn: {prediction}")
        
        return PredictionResponse(
            customer_id=customer.customer_id,
            churn_prediction=prediction,
            churn_probability=round(probability, 4),
            risk_level=risk_level,
            confidence=round(probability if prediction == 1 else 1 - probability, 4),
            timestamp=datetime.now().isoformat()
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erreur lors de la pr√©diction : {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Erreur lors de la pr√©diction : {str(e)}"
        )


@app.post("/predict/batch", response_model=BatchPredictionResponse, tags=["Prediction"])
async def predict_batch(request: BatchPredictionRequest):
    """
    Pr√©dictions pour plusieurs clients (batch)
    
    Parameters:
    -----------
    request : BatchPredictionRequest
        Liste de clients
        
    Returns:
    --------
    BatchPredictionResponse
        Pr√©dictions pour tous les clients
    """
    start_batch = time.time()
    
    try:
        # V√©rifier que le mod√®le est charg√©
        if model is None or preprocessor is None:
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="Mod√®le non charg√©"
            )
        
        # Pr√©dictions pour chaque client
        predictions = []
        high_risk_count = 0
        
        for customer in request.customers:
            # Pr√©parer donn√©es
            input_df = prepare_input_data(customer)
            input_processed = preprocessor.transform(input_df)
            
            # Pr√©diction
            prediction = int(model.predict(input_processed)[0])
            probability = float(model.predict_proba(input_processed)[0, 1])
            risk_level = calculate_risk_level(probability)
            
            if risk_level == "High":
                high_risk_count += 1
            
            predictions.append(PredictionResponse(
                customer_id=customer.customer_id,
                churn_prediction=prediction,
                churn_probability=round(probability, 4),
                risk_level=risk_level,
                confidence=round(probability if prediction == 1 else 1 - probability, 4),
                timestamp=datetime.now().isoformat()
            ))
        
        processing_time = (time.time() - start_batch) * 1000  # ms
        
        logger.info(f"Batch de {len(predictions)} pr√©dictions effectu√© en {processing_time:.2f}ms")
        
        return BatchPredictionResponse(
            predictions=predictions,
            total_customers=len(predictions),
            high_risk_count=high_risk_count,
            processing_time_ms=round(processing_time, 2),
            timestamp=datetime.now().isoformat()
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erreur lors du batch : {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Erreur lors du batch : {str(e)}"
        )


@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    """
    Gestionnaire global des exceptions
    """
    logger.error(f"Exception non g√©r√©e : {str(exc)}")
    
    return JSONResponse(
        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
        content={
            "error": "Internal Server Error",
            "detail": str(exc),
            "timestamp": datetime.now().isoformat()
        }
    )


if __name__ == "__main__":
    import uvicorn
    
    # Lancer le serveur
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8080,
        reload=True,
        log_level="info"
    )