"""
Monitoring ML avec Evidently AI
================================

Ce module utilise Evidently pour d√©tecter :
- D√©rive des donn√©es (Data Drift)
- D√©rive du mod√®le (Model Drift)
- D√©gradation des performances

"""

import pandas as pd
import numpy as np
from datetime import datetime
import json
import logging
from pathlib import Path
from typing import Dict, Tuple, Optional

from evidently import ColumnMapping
from evidently.report import Report
from evidently.metric_preset import (
    DataDriftPreset,
    DataQualityPreset,
    TargetDriftPreset,
    ClassificationPreset
)
from evidently.metrics import (
    DatasetDriftMetric,
    DatasetMissingValuesMetric,
    ClassificationQualityMetric,
    ClassificationClassBalance,
    ClassificationConfusionMatrix
)
from evidently.test_suite import TestSuite
from evidently.tests import (
    TestNumberOfDriftedColumns,
    TestShareOfDriftedColumns,
    TestAccuracyScore,
    TestPrecisionScore,
    TestRecallScore,
    TestF1Score
)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MLMonitor:
    """
    Classe pour monitorer les performances et la d√©rive du mod√®le
    """
    
    def __init__(
        self,
        reference_data: pd.DataFrame,
        model,
        preprocessor,
        target_col: str = 'churn',
        prediction_col: str = 'prediction',
        output_dir: str = '../monitoring/reports'
    ):
        """
        Initialisation du moniteur
        
        Parameters:
        -----------
        reference_data : pd.DataFrame
            Donn√©es de r√©f√©rence (train set)
        model : sklearn model
            Mod√®le entra√Æn√©
        preprocessor : DataPreprocessor
            Preprocessor entra√Æn√©
        target_col : str
            Nom de la colonne cible
        prediction_col : str
            Nom de la colonne de pr√©diction
        output_dir : str
            R√©pertoire de sortie des rapports
        """
        self.reference_data = reference_data.copy()
        self.model = model
        self.preprocessor = preprocessor
        self.target_col = target_col
        self.prediction_col = prediction_col
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Configuration des colonnes pour Evidently
        self.column_mapping = ColumnMapping(
            target=target_col,
            prediction=prediction_col,
            numerical_features=[
                'credit_score', 'age', 'tenure', 'balance',
                'products_number', 'estimated_salary'
            ],
            categorical_features=[
                'country', 'gender', 'credit_card', 'active_member'
            ]
        )
        
        # Ajouter les pr√©dictions aux donn√©es de r√©f√©rence
        if self.prediction_col not in self.reference_data.columns:
            self._add_predictions_to_reference()
        
        logger.info("‚úÖ MLMonitor initialis√©")
    
    def _add_predictions_to_reference(self):
        """Ajouter les pr√©dictions aux donn√©es de r√©f√©rence"""
        X_ref = self.reference_data.drop(columns=[self.target_col])
        X_ref_processed = self.preprocessor.transform(X_ref)
        predictions = self.model.predict(X_ref_processed)
        self.reference_data[self.prediction_col] = predictions
        logger.info("‚úÖ Pr√©dictions ajout√©es aux donn√©es de r√©f√©rence")
    
    def generate_data_drift_report(
        self,
        current_data: pd.DataFrame,
        save_html: bool = True
    ) -> Dict:
        """
        G√©n√©rer rapport de d√©rive des donn√©es
        
        Parameters:
        -----------
        current_data : pd.DataFrame
            Donn√©es actuelles (production)
        save_html : bool
            Sauvegarder en HTML
            
        Returns:
        --------
        dict : R√©sum√© de la d√©rive
        """
        logger.info("üìä G√©n√©ration du rapport Data Drift...")
        
        # Pr√©parer les donn√©es actuelles
        current_data = current_data.copy()
        
        # Ajouter pr√©dictions si n√©cessaire
        if self.prediction_col not in current_data.columns and self.target_col in current_data.columns:
            X_curr = current_data.drop(columns=[self.target_col])
            X_curr_processed = self.preprocessor.transform(X_curr)
            current_data[self.prediction_col] = self.model.predict(X_curr_processed)
        
        # Cr√©er le rapport
        report = Report(metrics=[
            DataDriftPreset(),
            DataQualityPreset(),
        ])
        
        report.run(
            reference_data=self.reference_data,
            current_data=current_data,
            column_mapping=self.column_mapping
        )
        
        # Sauvegarder HTML
        if save_html:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            html_path = self.output_dir / f"data_drift_report_{timestamp}.html"
            report.save_html(str(html_path))
            logger.info(f"‚úÖ Rapport HTML sauvegard√© : {html_path}")
        
        # Extraire les r√©sultats
        results = report.as_dict()
        logger.info(f"Resultats bruts drift: {results['metrics'][0]['result']}")
        res = results['metrics'][0]['result']
        
        # R√©sum√©
        drift_summary = {
            'timestamp': datetime.now().isoformat(),
            'dataset_drift_detected': res.get('dataset_drift', False),
            'number_of_drifted_columns': res.get('number_of_drifted_columns', 0),
            'share_of_drifted_columns': res.get('share_of_drifted_columns', 0.0),
            'drifted_columns': []
        }
        
        # D√©tails des colonnes qui ont d√©riv√©
        drift_cols = res.get('drift_by_columns', {})
        if not drift_cols:
          drift_cols = res.get('drift_columns', {})  
        for col_name, col_result in drift_cols.items():
          if col_result.get('drift_detected', False):
              drift_summary['drifted_columns'].append({
                'column': col_name,
                'drift_score': col_result.get('drift_score', 0),
                'stattest_name': col_result.get('stattest_name', 'unknown')
            })
        logger.info(f"üìà D√©rive d√©tect√©e : {drift_summary['dataset_drift_detected']}")
        logger.info(f"üìà Colonnes d√©riv√©es : {drift_summary['number_of_drifted_columns']}")
        return drift_summary
    
    def generate_model_performance_report(
        self,
        current_data: pd.DataFrame,
        save_html: bool = True
    ) -> Dict:
        """
        G√©n√©rer rapport de performance du mod√®le
        
        Parameters:
        -----------
        current_data : pd.DataFrame
            Donn√©es actuelles avec target et pr√©dictions
        save_html : bool
            Sauvegarder en HTML
            
        Returns:
        --------
        dict : M√©triques de performance
        """
        logger.info("üìä G√©n√©ration du rapport Model Performance...")
        
        # V√©rifier que target et predictions sont pr√©sents
        if self.target_col not in current_data.columns:
            raise ValueError(f"Colonne {self.target_col} manquante")
        
        current_data = current_data.copy()
        
        # Ajouter pr√©dictions si n√©cessaire
        if self.prediction_col not in current_data.columns:
            X_curr = current_data.drop(columns=[self.target_col])
            X_curr_processed = self.preprocessor.transform(X_curr)
            current_data[self.prediction_col] = self.model.predict(X_curr_processed)
        
        # Cr√©er le rapport
        report = Report(metrics=[
            ClassificationPreset(),
        ])
        
        report.run(
            reference_data=self.reference_data,
            current_data=current_data,
            column_mapping=self.column_mapping
        )
        
        # Sauvegarder HTML
        if save_html:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            html_path = self.output_dir / f"model_performance_report_{timestamp}.html"
            report.save_html(str(html_path))
            logger.info(f"‚úÖ Rapport HTML sauvegard√© : {html_path}")
        
        # Extraire les m√©triques
        results = report.as_dict()
        
        performance_summary = {
            'timestamp': datetime.now().isoformat(),
            'current_metrics': {},
            'reference_metrics': {}
        }
        
        # Extraire m√©triques (structure peut varier selon version Evidently)
        try:
            for metric in results['metrics']:
                if 'result' in metric:
                    if 'current' in metric['result']:
                        perf_current = metric['result']['current']
                        if isinstance(perf_current, dict):
                            performance_summary['current_metrics'].update(perf_current)
                    
                    if 'reference' in metric['result']:
                        perf_ref = metric['result']['reference']
                        if isinstance(perf_ref, dict):
                            performance_summary['reference_metrics'].update(perf_ref)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Extraction m√©triques incompl√®te : {e}")
        
        logger.info(f"üìà M√©triques actuelles : {performance_summary['current_metrics']}")
        
        return performance_summary
    
    def run_test_suite(
        self,
        current_data: pd.DataFrame,
        thresholds: Optional[Dict] = None
    ) -> Dict:
        """
        Ex√©cuter une suite de tests automatis√©s
        
        Parameters:
        -----------
        current_data : pd.DataFrame
            Donn√©es actuelles
        thresholds : dict, optional
            Seuils personnalis√©s
            
        Returns:
        --------
        dict : R√©sultats des tests
        """
        logger.info("üß™ Ex√©cution de la suite de tests...")
        
        if thresholds is None:
            thresholds = {
                'max_share_drifted_columns': 0.3,  # Max 30% colonnes d√©riv√©es
                'min_accuracy': 0.75,
                'min_precision': 0.60,
                'min_recall': 0.70,
                'min_f1': 0.65
            }
        
        # Pr√©parer les donn√©es
        current_data = current_data.copy()
        if self.prediction_col not in current_data.columns and self.target_col in current_data.columns:
            X_curr = current_data.drop(columns=[self.target_col])
            X_curr_processed = self.preprocessor.transform(X_curr)
            current_data[self.prediction_col] = self.model.predict(X_curr_processed)
        
        # Cr√©er la suite de tests
        test_suite = TestSuite(tests=[
            TestShareOfDriftedColumns(lt=thresholds['max_share_drifted_columns']),
            TestNumberOfDriftedColumns(lt=5),
        ])
        
        # Ajouter tests de performance si target disponible
        if self.target_col in current_data.columns:
            test_suite._tests.extend([
                TestAccuracyScore(gte=thresholds['min_accuracy']),
                TestPrecisionScore(gte=thresholds['min_precision']),
                TestRecallScore(gte=thresholds['min_recall']),
                TestF1Score(gte=thresholds['min_f1'])
            ])
        
        test_suite.run(
            reference_data=self.reference_data,
            current_data=current_data,
            column_mapping=self.column_mapping
        )
        
        # Sauvegarder r√©sultats
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        html_path = self.output_dir / f"test_suite_{timestamp}.html"
        test_suite.save_html(str(html_path))
        logger.info(f"‚úÖ Suite de tests sauvegard√©e : {html_path}")
        
        # Analyser r√©sultats
        results = test_suite.as_dict()
        
        test_summary = {
            'timestamp': datetime.now().isoformat(),
            'total_tests': len(results['tests']),
            'passed_tests': sum(1 for t in results['tests'] if t['status'] == 'SUCCESS'),
            'failed_tests': sum(1 for t in results['tests'] if t['status'] == 'FAIL'),
            'all_tests_passed': all(t['status'] == 'SUCCESS' for t in results['tests']),
            'failed_test_details': []
        }
        
        # D√©tails des tests √©chou√©s
        for test in results['tests']:
            if test['status'] == 'FAIL':
                test_summary['failed_test_details'].append({
                    'test_name': test['name'],
                    'description': test.get('description', ''),
                    'status': test['status']
                })
        
        logger.info(f"‚úÖ Tests r√©ussis : {test_summary['passed_tests']}/{test_summary['total_tests']}")
        if not test_summary['all_tests_passed']:
            logger.warning(f"‚ö†Ô∏è Tests √©chou√©s : {test_summary['failed_tests']}")
        
        return test_summary
    
    def check_need_for_retraining(
        self,
        current_data: pd.DataFrame,
        auto_threshold: bool = True
    ) -> Tuple[bool, str, Dict]:
        """
        V√©rifier si un r√©entra√Ænement est n√©cessaire
        
        Parameters:
        -----------
        current_data : pd.DataFrame
            Donn√©es de production actuelles
        auto_threshold : bool
            Utiliser seuils automatiques
            
        Returns:
        --------
        tuple : (needs_retraining, reason, details)
        """
        logger.info("üîç V√©rification besoin de r√©entra√Ænement...")
        
        # 1. V√©rifier d√©rive des donn√©es
        drift_summary = self.generate_data_drift_report(current_data, save_html=False)
        
        if drift_summary['dataset_drift_detected']:
            reason = f"D√©rive d√©tect√©e sur {drift_summary['number_of_drifted_columns']} colonnes"
            logger.warning(f"‚ö†Ô∏è {reason}")
            return True, reason, drift_summary
        
        # 2. V√©rifier performances si target disponible
        if self.target_col in current_data.columns:
            perf_summary = self.generate_model_performance_report(current_data, save_html=False)
            
            # V√©rifier si recall a chut√© (m√©trique cl√© pour le churn)
            current_recall = perf_summary['current_metrics'].get('recall', 1.0)
            reference_recall = perf_summary['reference_metrics'].get('recall', 1.0)
            
            if current_recall < 0.70:  # Seuil minimum
                reason = f"Recall trop faible : {current_recall:.3f} < 0.70"
                logger.warning(f"‚ö†Ô∏è {reason}")
                return True, reason, perf_summary
            
            # V√©rifier d√©gradation significative (>10%)
            if current_recall < reference_recall * 0.90:
                reason = f"D√©gradation du recall : {current_recall:.3f} vs {reference_recall:.3f}"
                logger.warning(f"‚ö†Ô∏è {reason}")
                return True, reason, perf_summary
        
        logger.info("‚úÖ Pas de besoin de r√©entra√Ænement d√©tect√©")
        return False, "Performances stables", {}
    
    def save_monitoring_summary(
        self,
        current_data: pd.DataFrame,
        output_file: str = None
    ):
        """
        Sauvegarder un r√©sum√© complet du monitoring
        """
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = self.output_dir / f"monitoring_summary_{timestamp}.json"
        
        logger.info("üíæ Sauvegarde du r√©sum√© de monitoring...")
        
        # Collecter toutes les informations
        drift_summary = self.generate_data_drift_report(current_data, save_html=False)
        
        summary = {
            'timestamp': datetime.now().isoformat(),
            'data_drift': drift_summary,
            'current_data_shape': current_data.shape,
            'reference_data_shape': self.reference_data.shape
        }
        
        # Ajouter performance si target disponible
        if self.target_col in current_data.columns:
            perf_summary = self.generate_model_performance_report(current_data, save_html=False)
            summary['model_performance'] = perf_summary
        
        # V√©rifier besoin retraining
        needs_retraining, reason, details = self.check_need_for_retraining(current_data)
        summary['retraining'] = {
            'needs_retraining': needs_retraining,
            'reason': reason,
            'details': details
        }
        
        # Sauvegarder
        with open(output_file, 'w') as f:
            json.dump(summary, f, indent=4)
        
        logger.info(f"‚úÖ R√©sum√© sauvegard√© : {output_file}")
        
        return summary


def main():
    """
    Exemple d'utilisation
    """
    import joblib
    import sys
    import os

    current_dir = os.path.dirname(os.path.abspath(__file__))
    models_dir = os.path.abspath(os.path.join(current_dir, '/Users/denismutombotshituka/bank-churn-mlops/src/models'))
    if models_dir not in sys.path:
       sys.path.insert(0, models_dir)

    from preprocessing import prepare_data_for_training
    
    print("\n" + "=" * 60)
    print("TEST DU MONITORING EVIDENTLY")
    print("=" * 60)
    
    # 1. Charger les donn√©es
    X_train, X_test, y_train, y_test, preprocessor = prepare_data_for_training(
        data_path='/Users/denismutombotshituka/bank-churn-mlops/data/raw/Bank_Churn_prediction.csv',
        target_col='churn',
        test_size=0.2,
        random_state=42
    )
    
    # 2. Charger le mod√®le
    model = joblib.load('/Users/denismutombotshituka/bank-churn-mlops/models/trained/model_latest.pkl')
    
    # 3. Pr√©parer les dataframes (avec features originales + target)
    df = pd.read_csv('/Users/denismutombotshituka/bank-churn-mlops/data/raw/Bank_Churn_prediction.csv')
    from sklearn.model_selection import train_test_split
    
    train_df, test_df = train_test_split(
        df, test_size=0.2, random_state=42, stratify=df['churn']
    )
    
    # 4. Initialiser le moniteur
    monitor = MLMonitor(
        reference_data=train_df,
        model=model,
        preprocessor=preprocessor,
        target_col='churn',
        prediction_col='prediction'
    )
    
    # 5. G√©n√©rer les rapports
    print("\nüìä G√©n√©ration des rapports...")
    drift_report = monitor.generate_data_drift_report(test_df)
    perf_report = monitor.generate_model_performance_report(test_df)
    test_results = monitor.run_test_suite(test_df)
    
    # 6. V√©rifier besoin de r√©entra√Ænement
    needs_retraining, reason, details = monitor.check_need_for_retraining(test_df)
    
    print(f"\n{'='*60}")
    print(f"R√âSULTATS DU MONITORING")
    print(f"{'='*60}")
    print(f"D√©rive d√©tect√©e : {drift_report['dataset_drift_detected']}")
    print(f"Tests r√©ussis : {test_results['passed_tests']}/{test_results['total_tests']}")
    print(f"R√©entra√Ænement n√©cessaire : {needs_retraining}")
    if needs_retraining:
        print(f"Raison : {reason}")
    
    # 7. Sauvegarder r√©sum√©
    monitor.save_monitoring_summary(test_df)
    
    print(f"\n‚úÖ Test du monitoring termin√©")
    print(f"üìÅ Rapports disponibles dans : {monitor.output_dir}")


if __name__ == "__main__":
    main()